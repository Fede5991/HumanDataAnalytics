{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "from scipy.io import wavfile\n",
    "import pandas as pd\n",
    "import random\n",
    "import time\n",
    "\n",
    "import speech_features as sf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataframe(root=\"./audio_files\", samples=-1, training_percentage=0.8,\n",
    "                     mean_normalization=True, nfilt=40, nfft=400, preemph=0.97,\n",
    "                     winlen=0.025, winstep=0.01, numcep=12, ceplifter=22,\n",
    "                     save=False, verbose=False, exclude=None):\n",
    "    \n",
    "    AUDIO_FILES_ROOT = root\n",
    "\n",
    "    folders = [f for f in os.listdir(AUDIO_FILES_ROOT) if os.path.isdir(AUDIO_FILES_ROOT + \"/\" + f) == True]\n",
    "    validation_list = [line.rstrip() for line in open('./audio_files/validation_list.txt')]\n",
    "    testing_list = [line.rstrip() for line in open('./audio_files/testing_list.txt')]\n",
    "        \n",
    "    if verbose:\n",
    "        print(\"### CLASSES ###\")\n",
    "        print(folders)\n",
    "        print(\"\\n\")\n",
    "        print(\"### DATASET SETTINGS ###\")\n",
    "        print(\"# of samples per class = \" + str(samples))\n",
    "        print(\"% of training samples = \" + str(training_percentage))\n",
    "        print(\"Length of the window = \" + str(winlen))\n",
    "        print(\"Step of the window = \" + str(winstep))\n",
    "        print(\"# of filters = \" + str(nfilt))\n",
    "        print(\"FFT size = \" + str(nfft))\n",
    "        print(\"Pre-emphasis coefficient = \" + str(preemph))\n",
    "        print(\"# of Cepstrum to return = \" + str(numcep))\n",
    "        print(\"Lifter coefficient = \" + str(ceplifter))\n",
    "        print(\"\\n\")\n",
    "        \n",
    "    # Silence creation\n",
    "    filename_column = []\n",
    "    class_column = []\n",
    "    duration_column = []\n",
    "    frequency_sampling_column = []\n",
    "    number_samples_column = []\n",
    "    samples_column = []\n",
    "    filter_banks_column = []\n",
    "    filter_banks_shape_column = []\n",
    "    mfcc_column = []\n",
    "    mfcc_shape_column = []\n",
    "    mfcc_deltas_column = []\n",
    "    mfcc_deltas_shape_column = []\n",
    "    ssc_column = []\n",
    "    ssc_shape_column = []\n",
    "    purpose_column = []\n",
    "    \n",
    "    noise_file_path = AUDIO_FILES_ROOT + \"/_background_noise_/\"\n",
    "\n",
    "    noise_wav_files = [wavfile.read(noise_file_path + f) for f in os.listdir(noise_file_path) if f[f.find('.'):] == \".wav\"]\n",
    "    normalized_wav_files = []\n",
    "\n",
    "    for file in noise_wav_files:\n",
    "        normalized_wav_files.append(file[1] / 32767)\n",
    "        \n",
    "    key = 'silence'\n",
    "    min_noise_sound = 0.05\n",
    "    max_noise_sound = 0.3\n",
    "    \n",
    "    if samples == -1:\n",
    "        silence_max = 2000\n",
    "    else:\n",
    "        silence_max = samples\n",
    "    values = []\n",
    "    fs = 16000\n",
    "    \n",
    "    silence_training = training_percentage * silence_max\n",
    "    silence_validation = (silence_max - silence_training) / 2\n",
    "    silence_test = silence_validation\n",
    "    s_training_counter = 0\n",
    "    s_validation_counter = 0\n",
    "    s_test_counter = 0\n",
    "\n",
    "    if verbose:\n",
    "        print (\"### silence (\" + str(silence_max) + \" samples) ###\")\n",
    "    for i in range(silence_max):\n",
    "        #noise = functions.noiseSelector(noiseDict, sampleRate)\n",
    "        bn_index = random.randint(0,len(noise_wav_files)-1)\n",
    "        background_noise = normalized_wav_files[bn_index]\n",
    "\n",
    "        initial_index = random.randint(0, len(background_noise) - fs)\n",
    "        final_index = initial_index + fs\n",
    "        noise = background_noise[initial_index:final_index]\n",
    "\n",
    "        sig = noise * np.random.uniform(high = max_noise_sound, low = min_noise_sound)\n",
    "        \n",
    "        fbank_feat = sf.logfbank(sig, fs, winlen=winlen, winstep=winstep, nfilt=nfilt, nfft=nfft, preemph=preemph)\n",
    "        \n",
    "        mfcc_feat = sf.mfcc(sig, fs, winlen=winlen, winstep=winstep, nfilt=nfilt, nfft=nfft, preemph=preemph,\n",
    "                            numcep=numcep, ceplifter=ceplifter)\n",
    "        mfcc_delta_feat = sf.delta(mfcc_feat, 2)\n",
    "        mfcc_delta_delta_feat = sf.delta(mfcc_delta_feat, 2)\n",
    "        mfcc_complete = np.concatenate((np.concatenate((mfcc_feat, mfcc_delta_feat),axis=1), mfcc_delta_delta_feat),axis=1)\n",
    "        \n",
    "        ssc_feat = sf.ssc(sig, fs, winlen=winlen, winstep=winstep, nfilt=nfilt, nfft=nfft, preemph=preemph)\n",
    "        \n",
    "        if mean_normalization:\n",
    "            fbank_feat -= (np.mean(fbank_feat, axis=0) + 1e-8)\n",
    "            mfcc_feat -= (np.mean(mfcc_feat, axis=0) + 1e-8)\n",
    "        \n",
    "        number_of_samples = len(sig)\n",
    "        duration = number_of_samples / fs\n",
    "        \n",
    "        if i < silence_training:\n",
    "            purpose = \"training\"\n",
    "            s_training_counter += 1\n",
    "        elif i < silence_training + silence_validation:\n",
    "            purpose = \"validation\"\n",
    "            s_validation_counter += 1\n",
    "        else:\n",
    "            purpose = \"testing\"\n",
    "            s_test_counter += 1\n",
    "        \n",
    "        filename_column.append(\"\")\n",
    "        class_column.append(key)\n",
    "        duration_column.append(duration)\n",
    "        frequency_sampling_column.append(fs)\n",
    "        number_samples_column.append(number_of_samples)\n",
    "        samples_column.append(np.array(sig))\n",
    "        filter_banks_column.append(np.matrix(fbank_feat))\n",
    "        filter_banks_shape_column.append(fbank_feat.shape)\n",
    "        mfcc_column.append(np.matrix(mfcc_feat))\n",
    "        mfcc_shape_column.append(mfcc_feat.shape)\n",
    "        mfcc_deltas_column.append(np.matrix(mfcc_complete))\n",
    "        mfcc_deltas_shape_column.append(mfcc_complete.shape)\n",
    "        ssc_column.append(np.matrix(ssc_feat))\n",
    "        ssc_shape_column.append(ssc_feat.shape)\n",
    "        purpose_column.append(purpose)\n",
    "    \n",
    "    if verbose:\n",
    "        print(\" - created \" + str(s_training_counter) + \" training samples, \" + str(s_validation_counter) + \n",
    "              \" validation samples and \" + str(s_test_counter) + \" test samples\")\n",
    "    \n",
    "    for folder in folders:\n",
    "        if folder in exclude:\n",
    "            if verbose:\n",
    "                print(\"### \" + folder + \" NOT USED ###\")\n",
    "        else:\n",
    "            c_training_counter = 0\n",
    "            c_validation_counter = 0\n",
    "            c_test_counter = 0\n",
    "            wav_files = [f for f in os.listdir(AUDIO_FILES_ROOT + \"/\" + folder) if f[f.find('.'):] == \".wav\"]\n",
    "            if verbose:\n",
    "                print(\"### \" + folder + \" (\" + str(len(wav_files)) + \" files) ###\")\n",
    "            for wav_file in wav_files:\n",
    "                file_path = AUDIO_FILES_ROOT + \"/\" + folder + \"/\" + wav_file\n",
    "                fs, signal = wavfile.read(file_path)\n",
    "                signal = signal / 32767\n",
    "                number_of_samples = len(signal)\n",
    "                duration = number_of_samples / fs\n",
    "                \n",
    "                if duration != 1:\n",
    "                    s = np.zeros(fs)\n",
    "                    needed_zeros = fs - number_of_samples\n",
    "                    offset = needed_zeros // 2\n",
    "                    s[offset:number_of_samples + offset] = signal\n",
    "                    signal = s\n",
    "                    number_of_samples = len(signal)\n",
    "                    duration = number_of_samples / fs\n",
    "                    \n",
    "                fbank_feat = sf.logfbank(sig, fs, winlen=winlen, winstep=winstep, nfilt=nfilt, nfft=nfft, preemph=preemph)\n",
    "        \n",
    "                mfcc_feat = sf.mfcc(sig, fs, winlen=winlen, winstep=winstep, nfilt=nfilt, nfft=nfft, preemph=preemph,\n",
    "                                    numcep=numcep, ceplifter=ceplifter)\n",
    "                mfcc_delta_feat = sf.delta(mfcc_feat, 2)\n",
    "                mfcc_delta_delta_feat = sf.delta(mfcc_delta_feat, 2)\n",
    "                mfcc_complete = np.concatenate((np.concatenate((mfcc_feat, mfcc_delta_feat),axis=1), mfcc_delta_delta_feat),axis=1)\n",
    "\n",
    "                ssc_feat = sf.ssc(sig, fs, winlen=winlen, winstep=winstep, nfilt=nfilt, nfft=nfft, preemph=preemph)\n",
    "\n",
    "                if mean_normalization:\n",
    "                    fbank_feat -= (np.mean(fbank_feat, axis=0) + 1e-8)\n",
    "                    mfcc_feat -= (np.mean(mfcc_feat, axis=0) + 1e-8)\n",
    "                \n",
    "                purpose = \"\"\n",
    "                if (folder + \"/\" + wav_file) in validation_list:\n",
    "                    purpose = \"validation\"\n",
    "                    c_validation_counter += 1\n",
    "                elif (folder + \"/\" + wav_file) in testing_list:\n",
    "                    purpose = \"testing\"\n",
    "                    c_test_counter += 1\n",
    "                else:\n",
    "                    purpose = \"training\"\n",
    "                    c_training_counter += 1\n",
    "                    \n",
    "                filename_column.append(wav_file)\n",
    "                class_column.append(folder)\n",
    "                duration_column.append(duration)\n",
    "                frequency_sampling_column.append(fs)\n",
    "                number_samples_column.append(number_of_samples)\n",
    "                samples_column.append(np.array(signal))\n",
    "                filter_banks_column.append(np.matrix(fbank_feat))\n",
    "                filter_banks_shape_column.append(fbank_feat.shape)\n",
    "                mfcc_column.append(np.matrix(mfcc_feat))\n",
    "                mfcc_shape_column.append(mfcc_feat.shape)\n",
    "                mfcc_deltas_column.append(np.matrix(mfcc_complete))\n",
    "                mfcc_deltas_shape_column.append(mfcc_complete.shape)\n",
    "                ssc_column.append(np.matrix(ssc_feat))\n",
    "                ssc_shape_column.append(ssc_feat.shape)\n",
    "                purpose_column.append(purpose)\n",
    "            if verbose:\n",
    "                print(\" - created \" + str(c_training_counter) + \" training samples, \" + str(c_validation_counter) + \n",
    "                      \" validation samples and \" + str(c_test_counter) + \" test samples\")\n",
    "    \n",
    "    d = {'filename': filename_column, 'class': class_column, 'duration': duration_column,\n",
    "         'frequency sampling': frequency_sampling_column, 'number of samples': number_samples_column,\n",
    "         'samples': samples_column, 'filter banks': filter_banks_column, 'filter banks shape': filter_banks_shape_column,\n",
    "         'mfcc': mfcc_column, 'mfcc shape': mfcc_shape_column,\n",
    "         'mfcc deltas': mfcc_deltas_column, 'mfcc deltas shape': mfcc_deltas_shape_column,\n",
    "         'ssc': ssc_column, 'ssc shape': ssc_shape_column,\n",
    "         'purpose': purpose_column}\n",
    "    df = pd.DataFrame(data=d)\n",
    "    \n",
    "    filename_column = []\n",
    "    class_column = []\n",
    "    duration_column = []\n",
    "    frequency_sampling_column = []\n",
    "    number_samples_column = []\n",
    "    samples_column = []\n",
    "    filter_banks_column = []\n",
    "    filter_banks_shape_column = []\n",
    "    mfcc_column = []\n",
    "    mfcc_shape_column = []\n",
    "    mfcc_deltas_column = []\n",
    "    mfcc_deltas_shape_column = []\n",
    "    ssc_column = []\n",
    "    ssc_shape_column = []\n",
    "    purpose_column = []\n",
    "    \n",
    "    min_noise_sound = 0.05\n",
    "    max_noise_sound = 0.2\n",
    "    \n",
    "    fs = 16000\n",
    "    \n",
    "    samples_training = training_percentage * samples\n",
    "    samples_validation = (samples - samples_training) / 2\n",
    "    samples_test = samples_validation\n",
    "    \n",
    "    print(\"### Creating new training samples ###\")\n",
    "    df_train = df[df[\"purpose\"] == \"training\"]\n",
    "    for name, group in df_train.groupby([\"class\"]):\n",
    "        \n",
    "        needed_samples = int(samples_training - len(group))\n",
    "        df_samples = group[\"samples\"].values\n",
    "        \n",
    "        print(\" - \" + name + \", needed samples: \" + str(needed_samples))\n",
    "        \n",
    "        for i in range(needed_samples):\n",
    "            sig_index = random.randint(0,len(df_samples)-1)\n",
    "            signal = df_samples[sig_index]\n",
    "\n",
    "            bn_index = random.randint(0,len(noise_wav_files)-1)\n",
    "            background_noise = normalized_wav_files[bn_index]\n",
    "            initial_index = random.randint(0, len(background_noise) - fs)\n",
    "            final_index = initial_index + fs\n",
    "            noise = background_noise[initial_index:final_index]\n",
    "\n",
    "            sig = signal + (noise * np.random.uniform(high = max_noise_sound, low = min_noise_sound))\n",
    "            \n",
    "            fbank_feat = sf.logfbank(sig, fs, winlen=winlen, winstep=winstep, nfilt=nfilt, nfft=nfft, preemph=preemph)\n",
    "        \n",
    "            mfcc_feat = sf.mfcc(sig, fs, winlen=winlen, winstep=winstep, nfilt=nfilt, nfft=nfft, preemph=preemph,\n",
    "                                numcep=numcep, ceplifter=ceplifter)\n",
    "            mfcc_delta_feat = sf.delta(mfcc_feat, 2)\n",
    "            mfcc_delta_delta_feat = sf.delta(mfcc_delta_feat, 2)\n",
    "            mfcc_complete = np.concatenate((np.concatenate((mfcc_feat, mfcc_delta_feat),axis=1), mfcc_delta_delta_feat),axis=1)\n",
    "\n",
    "            ssc_feat = sf.ssc(sig, fs, winlen=winlen, winstep=winstep, nfilt=nfilt, nfft=nfft, preemph=preemph)\n",
    "\n",
    "            if mean_normalization:\n",
    "                fbank_feat -= (np.mean(fbank_feat, axis=0) + 1e-8)\n",
    "                mfcc_feat -= (np.mean(mfcc_feat, axis=0) + 1e-8)\n",
    "            \n",
    "            number_of_samples = len(signal)\n",
    "            duration = number_of_samples / fs\n",
    "            \n",
    "            filename_column.append(\"\")\n",
    "            class_column.append(name)\n",
    "            duration_column.append(duration)\n",
    "            frequency_sampling_column.append(fs)\n",
    "            number_samples_column.append(number_of_samples)\n",
    "            samples_column.append(np.array(sig))\n",
    "            filter_banks_column.append(np.matrix(fbank_feat))\n",
    "            filter_banks_shape_column.append(fbank_feat.shape)\n",
    "            mfcc_column.append(np.matrix(mfcc_feat))\n",
    "            mfcc_shape_column.append(mfcc_feat.shape)\n",
    "            mfcc_deltas_column.append(np.matrix(mfcc_complete))\n",
    "            mfcc_deltas_shape_column.append(mfcc_complete.shape)\n",
    "            ssc_column.append(np.matrix(ssc_feat))\n",
    "            ssc_shape_column.append(ssc_feat.shape)\n",
    "            purpose_column.append(\"training\")\n",
    "            \n",
    "    print(\"### Creating new validation samples ###\")\n",
    "    df_validation = df[df[\"purpose\"] == \"validation\"]\n",
    "    for name, group in df_validation.groupby([\"class\"]):\n",
    "        \n",
    "        needed_samples = int(samples_validation - len(group))\n",
    "        df_samples = group[\"samples\"].values\n",
    "        \n",
    "        print(\" - \" + name + \", needed samples: \" + str(needed_samples))\n",
    "        \n",
    "        for i in range(needed_samples):\n",
    "            sig_index = random.randint(0,len(df_samples)-1)\n",
    "            signal = df_samples[sig_index]\n",
    "\n",
    "            bn_index = random.randint(0,len(noise_wav_files)-1)\n",
    "            background_noise = normalized_wav_files[bn_index]\n",
    "            initial_index = random.randint(0, len(background_noise) - fs)\n",
    "            final_index = initial_index + fs\n",
    "            noise = background_noise[initial_index:final_index]\n",
    "\n",
    "            sig = signal + (noise * np.random.uniform(high = max_noise_sound, low = min_noise_sound))\n",
    "            \n",
    "            fbank_feat = sf.logfbank(sig, fs, winlen=winlen, winstep=winstep, nfilt=nfilt, nfft=nfft, preemph=preemph)\n",
    "        \n",
    "            mfcc_feat = sf.mfcc(sig, fs, winlen=winlen, winstep=winstep, nfilt=nfilt, nfft=nfft, preemph=preemph,\n",
    "                                numcep=numcep, ceplifter=ceplifter)\n",
    "            mfcc_delta_feat = sf.delta(mfcc_feat, 2)\n",
    "            mfcc_delta_delta_feat = sf.delta(mfcc_delta_feat, 2)\n",
    "            mfcc_complete = np.concatenate((np.concatenate((mfcc_feat, mfcc_delta_feat),axis=1), mfcc_delta_delta_feat),axis=1)\n",
    "\n",
    "            ssc_feat = sf.ssc(sig, fs, winlen=winlen, winstep=winstep, nfilt=nfilt, nfft=nfft, preemph=preemph)\n",
    "\n",
    "            if mean_normalization:\n",
    "                fbank_feat -= (np.mean(fbank_feat, axis=0) + 1e-8)\n",
    "                mfcc_feat -= (np.mean(mfcc_feat, axis=0) + 1e-8)\n",
    "            \n",
    "            number_of_samples = len(signal)\n",
    "            duration = number_of_samples / fs\n",
    "            \n",
    "            filename_column.append(\"\")\n",
    "            class_column.append(name)\n",
    "            duration_column.append(duration)\n",
    "            frequency_sampling_column.append(fs)\n",
    "            number_samples_column.append(number_of_samples)\n",
    "            samples_column.append(np.array(sig))\n",
    "            filter_banks_column.append(np.matrix(fbank_feat))\n",
    "            filter_banks_shape_column.append(fbank_feat.shape)\n",
    "            mfcc_column.append(np.matrix(mfcc_feat))\n",
    "            mfcc_shape_column.append(mfcc_feat.shape)\n",
    "            mfcc_deltas_column.append(np.matrix(mfcc_complete))\n",
    "            mfcc_deltas_shape_column.append(mfcc_complete.shape)\n",
    "            ssc_column.append(np.matrix(ssc_feat))\n",
    "            ssc_shape_column.append(ssc_feat.shape)\n",
    "            purpose_column.append(\"validation\")\n",
    "            \n",
    "    print(\"### Creating new test samples ###\")\n",
    "    df_test = df[df[\"purpose\"] == \"testing\"]\n",
    "    for name, group in df_test.groupby([\"class\"]):\n",
    "        \n",
    "        needed_samples = int(samples_test - len(group))\n",
    "        df_samples = group[\"samples\"].values\n",
    "        \n",
    "        print(\" - \" + name + \", needed samples: \" + str(needed_samples))\n",
    "        \n",
    "        for i in range(needed_samples):\n",
    "            sig_index = random.randint(0,len(df_samples)-1)\n",
    "            signal = df_samples[sig_index]\n",
    "\n",
    "            bn_index = random.randint(0,len(noise_wav_files)-1)\n",
    "            background_noise = normalized_wav_files[bn_index]\n",
    "            initial_index = random.randint(0, len(background_noise) - fs)\n",
    "            final_index = initial_index + fs\n",
    "            noise = background_noise[initial_index:final_index]\n",
    "\n",
    "            sig = signal + (noise * np.random.uniform(high = max_noise_sound, low = min_noise_sound))\n",
    "            \n",
    "            fbank_feat = sf.logfbank(sig, fs, winlen=winlen, winstep=winstep, nfilt=nfilt, nfft=nfft, preemph=preemph)\n",
    "        \n",
    "            mfcc_feat = sf.mfcc(sig, fs, winlen=winlen, winstep=winstep, nfilt=nfilt, nfft=nfft, preemph=preemph,\n",
    "                                numcep=numcep, ceplifter=ceplifter)\n",
    "            mfcc_delta_feat = sf.delta(mfcc_feat, 2)\n",
    "            mfcc_delta_delta_feat = sf.delta(mfcc_delta_feat, 2)\n",
    "            mfcc_complete = np.concatenate((np.concatenate((mfcc_feat, mfcc_delta_feat),axis=1), mfcc_delta_delta_feat),axis=1)\n",
    "\n",
    "            ssc_feat = sf.ssc(sig, fs, winlen=winlen, winstep=winstep, nfilt=nfilt, nfft=nfft, preemph=preemph)\n",
    "            \n",
    "            if mean_normalization:\n",
    "                fbank_feat -= (np.mean(fbank_feat, axis=0) + 1e-8)\n",
    "                mfcc_feat -= (np.mean(mfcc_feat, axis=0) + 1e-8)\n",
    "            \n",
    "            number_of_samples = len(signal)\n",
    "            duration = number_of_samples / fs\n",
    "            \n",
    "            filename_column.append(\"\")\n",
    "            class_column.append(name)\n",
    "            duration_column.append(duration)\n",
    "            frequency_sampling_column.append(fs)\n",
    "            number_samples_column.append(number_of_samples)\n",
    "            samples_column.append(np.array(sig))\n",
    "            filter_banks_column.append(np.matrix(fbank_feat))\n",
    "            filter_banks_shape_column.append(fbank_feat.shape)\n",
    "            mfcc_column.append(np.matrix(mfcc_feat))\n",
    "            mfcc_shape_column.append(mfcc_feat.shape)\n",
    "            mfcc_deltas_column.append(np.matrix(mfcc_complete))\n",
    "            mfcc_deltas_shape_column.append(mfcc_complete.shape)\n",
    "            ssc_column.append(np.matrix(ssc_feat))\n",
    "            ssc_shape_column.append(ssc_feat.shape)\n",
    "            purpose_column.append(\"testing\")\n",
    "            \n",
    "    d2 = {'filename': filename_column, 'class': class_column, 'duration': duration_column,\n",
    "         'frequency sampling': frequency_sampling_column, 'number of samples': number_samples_column,\n",
    "         'samples': samples_column, 'filter banks': filter_banks_column, 'filter banks shape': filter_banks_shape_column,\n",
    "         'mfcc': mfcc_column, 'mfcc shape': mfcc_shape_column,\n",
    "         'mfcc deltas': mfcc_deltas_column, 'mfcc deltas shape': mfcc_deltas_shape_column,\n",
    "         'ssc': ssc_column, 'ssc shape': ssc_shape_column,\n",
    "         'purpose': purpose_column}\n",
    "    df2 = pd.DataFrame(data=d2)\n",
    "    \n",
    "    df = df.append(df2, ignore_index = True)\n",
    "    \n",
    "    folders.append(\"silence\")\n",
    "    \n",
    "    settings = [samples, training_percentage, folders,\n",
    "                mean_normalization, nfilt, nfft, preemph,\n",
    "                winlen, winstep, numcep, ceplifter]\n",
    "    \n",
    "    if save:\n",
    "        print(\"Saving file...\")\n",
    "        df.to_hdf('dataframe.h5', key='df', mode='w')\n",
    "        print(\"Done!\")\n",
    "        \n",
    "    return df, settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_dataset(dataframe, settings, path = \"./dataset\", coefficients = [\"filter banks\", \"mfcc\", \"mfcc deltas\", \"ssc\"]):\n",
    "    \n",
    "    millis = int(round(time.time() * 1000))\n",
    "    dataset_folder = millis\n",
    "    \n",
    "    file = open(path + \"/\" + str(dataset_folder) + \"/settings.txt\", \"w\") \n",
    "    file.write(\"### DATASET SETTINGS ###\")\n",
    "    file.write(\"# of samples per class = \" + str(settings[0]))\n",
    "    file.write(\"% of training samples = \" + str(settings[1]))\n",
    "    file.write(\"Classes: \" + str(settings[2]))\n",
    "    file.write(\"Length of the window = \" + str(settings[7]))\n",
    "    file.write(\"Step of the window = \" + str(settings[8]))\n",
    "    file.write(\"# of filters = \" + str(settings[4]))\n",
    "    file.write(\"FFT size = \" + str(settings[5]))\n",
    "    file.write(\"Pre-emphasis coefficient = \" + str(settings[6]))\n",
    "    file.write(\"# of Cepstrum to return = \" + str(settings[9]))\n",
    "    file.write(\"Lifter coefficient = \" + str(settings[10]))\n",
    "    file.write(\"Mean normalization: \" + str(settings[3]))\n",
    "    file.close() \n",
    "    \n",
    "    for coeff in coefficients:\n",
    "        filename_coeff = coeff.replace(\" \", \"_\")\n",
    "\n",
    "        print(\"***\" + coeff.upper() + \"***\")\n",
    "\n",
    "        print(\"### TRAINING ###\")\n",
    "        print(\"- dataframe filtering\")\n",
    "        df_train = dataframe[dataframe.purpose == \"training\"]\n",
    "        X_train_raw = df_train[coeff].values\n",
    "        X_train = []\n",
    "        print(\"- X_train creation\")\n",
    "        for mat in X_train_raw:\n",
    "            X_train.append(mat)\n",
    "        X_train = np.array(X_train)\n",
    "        print(\"- y_train creation\")\n",
    "        y_train = (pd.get_dummies(df_train[\"class\"])).values\n",
    "\n",
    "        print(\"X_train shape: \", X_train.shape)\n",
    "        print(\"y_train shape: \", y_train.shape)\n",
    "\n",
    "        print(\"### VALIDATION ###\")\n",
    "        print(\"- dataframe filtering\")\n",
    "        df_validation = dataframe[dataframe.purpose == \"validation\"]\n",
    "        X_validation_raw = df_validation[coeff].values\n",
    "        X_validation = []\n",
    "        print(\"- X_train creation\")\n",
    "        for mat in X_validation_raw:\n",
    "            X_validation.append(mat)\n",
    "        X_validation = np.array(X_validation)\n",
    "        print(\"- y_train creation\")\n",
    "        y_validation = (pd.get_dummies(df_validation[\"class\"])).values\n",
    "\n",
    "        print(\"X_validation shape: \", X_validation.shape)\n",
    "        print(\"y_validation shape: \", y_validation.shape)\n",
    "\n",
    "        print(\"### TESTING ###\")\n",
    "        print(\"- dataframe filtering\")\n",
    "        df_test = dataframe[dataframe.purpose == \"testing\"]\n",
    "        X_test_raw = df_test[coeff].values\n",
    "        X_test = []\n",
    "        print(\"- X_test creation\")\n",
    "        for mat in X_test_raw:\n",
    "            X_test.append(mat)\n",
    "        X_test = np.array(X_test)\n",
    "        print(\"- y_test creation\")\n",
    "        y_test = (pd.get_dummies(df_test[\"class\"])).values\n",
    "\n",
    "        print(\"X_test shape: \", X_test.shape)\n",
    "        print(\"y_test shape: \", y_test.shape)\n",
    "\n",
    "        print(\" - saving files\")\n",
    "        np.save(path + \"/\" + str(dataset_folder) + \"/\" + filename_coeff + \"_training_set.npy\", X_train)\n",
    "        np.save(path + \"/\" + str(dataset_folder) + \"/\" + filename_coeff + \"_validation_set.npy\", X_validation)\n",
    "        np.save(path + \"/\" + str(dataset_folder) + \"/\" + filename_coeff + \"_test_set.npy\", X_test)\n",
    "\n",
    "        np.save(path + \"/\" + str(dataset_folder) + \"/\" + filename_coeff + \"_training_labels.npy\", y_train)\n",
    "        np.save(path + \"/\" + str(dataset_folder) + \"/\" + filename_coeff + \"_validation_labels.npy\", y_validation)\n",
    "        np.save(path + \"/\" + str(dataset_folder) + \"/\" + filename_coeff + \"_test_labels.npy\", y_test)\n",
    "\n",
    "        print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df, settings = create_dataframe(verbose=True, samples=3000, exclude=[\"_background_noise_\"])\n",
    "save_dataset(df, settings)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
