{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Speech recognition - CNN1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.io import wavfile\n",
    "import plotly.graph_objects as go\n",
    "import plotly\n",
    "from plotly.subplots import make_subplots\n",
    "import winsound\n",
    "import time\n",
    "from keras.utils.vis_utils import plot_model\n",
    "\n",
    "from IAHOS import IAHOS\n",
    "from extraction_performances import extraction_performances\n",
    "from hyperparams_initialization import hyperparams_initialization\n",
    "from plots import plot_IAHOS,plot_confusion_matrix\n",
    "from plots import plot_training_accuracy,plot_validation_accuracy,plot_test_scores\n",
    "from plots import plot_output_NN\n",
    "\n",
    "import keras\n",
    "from keras import backend as K\n",
    "from keras.models import Sequential,Input,Model,load_model\n",
    "from keras.layers import Conv2D,Conv1D,MaxPooling2D,AveragePooling1D,MaxPooling1D\n",
    "from keras.layers import Dense,Flatten,Dropout\n",
    "from keras import initializers,optimizers,backend as k\n",
    "from keras_radam import RAdam\n",
    "from keras.callbacks import EarlyStopping,ModelCheckpoint\n",
    "from sklearn.metrics import confusion_matrix,accuracy_score\n",
    "import tensorflow as tf\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Classifier(i, network_input_shape, number_of_classes, feature, hm, optimizer, op, mode):\n",
    "    \n",
    "    # Network definition\n",
    "    x = Input(shape=(int(network_input_shape[0]), int(network_input_shape[1]), 1))\n",
    "    \n",
    "    x1 = (Conv2D(int(hm[0,i]), kernel_size=(int(network_input_shape[0]), int(hm[1,i])),activation='relu'))(x)\n",
    "    \n",
    "    #if feature == \"mfcc_deltas\":\n",
    "    #    x1 = (Conv2D(int(hm[0,i]), kernel_size=(int(network_input_shape[0]/3), int(hm[1,i])),activation='relu'))(x)\n",
    "    #else:   \n",
    "\n",
    "    if mode == \"test\":\n",
    "        x1 = Dropout(rate=0)(x1)\n",
    "    else:\n",
    "        x1 = Dropout(rate=hm[8,i])(x1)\n",
    "    \n",
    "    x1 = (MaxPooling2D(pool_size=(1,int(hm[2,i]))))(x1)\n",
    "    \n",
    "    for j in range(int(hm[3,i])):\n",
    "        x1 = (Conv2D(int(hm[4,i]),kernel_size=(1,int(hm[5,i])),activation='relu'))(x1)\n",
    "        if mode == \"test\":\n",
    "            x1 = Dropout(rate=0)(x1)\n",
    "        else:\n",
    "            x1 = Dropout(rate=hm[8,i])(x1)\n",
    "        x1 = (MaxPooling2D(pool_size=(1,int(hm[6,i]))))(x1)\n",
    "        \n",
    "    y1 = Flatten()(x1)\n",
    "    y2 = (Dense(int(hm[7,i]),activation='relu'))(y1)\n",
    "    y2 = Dropout(rate=hm[8,i])(y2)\n",
    "    y = (Dense(number_of_classes,activation='softmax'))(y2)\n",
    "    \n",
    "    classifier = Model(inputs=x,outputs=y)\n",
    "    \n",
    "    # Optimizer choice\n",
    "    if optimizer=='radam':\n",
    "        Optimizer=RAdam()\n",
    "    elif optimizer=='sgd':\n",
    "        if op[3]==0:\n",
    "            Optimizer=keras.optimizers.SGD(lr=0.01, momentum=0.0, decay=0.0, nesterov=False)\n",
    "        else:\n",
    "            Optimizer=keras.optimizers.SGD(lr=0.01, momentum=0.0, decay=0.0, nesterov=True)\n",
    "    elif optimizer=='rmsprop':\n",
    "        Optimizer=keras.optimizers.RMSprop(lr=0.001, rho=0.9, epsilon=None, decay=0.0)\n",
    "    elif optimizer=='adagrad':\n",
    "        Optimizer=keras.optimizers.Adagrad(lr=0.01, epsilon=None, decay=0.0)\n",
    "    elif optimizer=='adadelta':\n",
    "        Optimizer=keras.optimizers.Adadelta(lr=1.0, rho=0.95,epsilon=None, decay=0.0)\n",
    "    elif optimizer=='adam':\n",
    "        Optimizer=keras.optimizers.Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0, amsgrad=False)\n",
    "    elif optimizer=='adamax':\n",
    "        Optimizer=keras.optimizers.Adamax(lr=0.002, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0)\n",
    "    else:\n",
    "        Optimizer=keras.optimizers.Nadam(lr=0.002, beta_1=0.9, beta_2=0.999, epsilon=None, schedule_decay=0.004)\n",
    "    classifier.compile(optimizer=Optimizer,loss='categorical_crossentropy',metrics=['accuracy'])\n",
    "    \n",
    "    return classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_a_number(s):\n",
    "    try: \n",
    "        int(s)\n",
    "        return True\n",
    "    except ValueError:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "network = \"CNN1\"\n",
    "features = [\"filter_banks\", \"mfcc\", \"mfcc_deltas\"]\n",
    "dataset_folder = './dataset/'\n",
    "results_folder = \"./results/\"\n",
    "version_folder = \"1567874784415\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attempts = 0\n",
    "rounds = 0\n",
    "variables = 9\n",
    "iterations = attempts**variables\n",
    "\n",
    "limits = [[4,64],    #0 number of filters in the first convolutional layer\n",
    "          [2,6],     #1 y component of the kernel in the first convolutional layer\n",
    "          [1,2],     #2 y component of the max pooling layer in the first convolutional layer\n",
    "          [0,2],     #3 number of additional convolutional layers to add in the network\n",
    "          [2,64],    #4 number of filters in the next convolutional layers\n",
    "          [2,6],     #5 y component of the kernel in the next convolutional layers\n",
    "          [1,2],     #6 y component of the max pooling layer in the next convolutional layers\n",
    "          [50,500],  #7 number of neurons in the fully connected later\n",
    "          [0,0.5]]   #8 dropout parameter\n",
    "\n",
    "op = [[],                         #0 RAdam\n",
    "      [0.01,0,0,0],               #1 SGD \n",
    "      [0.001,0.9,0,0],            #2 RMSprop\n",
    "      [0.01,0,0],                 #3 Adagrad\n",
    "      [1,0.95,0,0],               #4 Adadelta\n",
    "      [0.001,0.9,0.999,0,0,0],    #5 Adam\n",
    "      [0.002,0.9,0.999,0,0],      #6 Adamax\n",
    "      [0.002,0.9,0.999,0,0.004]]  #7 Nadam\n",
    "\n",
    "method = 'grid'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for feature in features:\n",
    "    \n",
    "    print(\"### \" + feature.upper() + \" ###\")\n",
    "    \n",
    "    dir_name = results_folder + version_folder\n",
    "    if not os.path.exists(dir_name):\n",
    "        os.mkdir(dir_name)\n",
    "        \n",
    "    dir_name = dir_name + \"/\" + network\n",
    "    if not os.path.exists(dir_name):\n",
    "        os.mkdir(dir_name)\n",
    "    \n",
    "    dir_name = dir_name + \"/\" + feature\n",
    "    if not os.path.exists(dir_name):\n",
    "        os.mkdir(dir_name)\n",
    "        output_folder = dir_name + \"/1\"\n",
    "        os.mkdir(output_folder)\n",
    "    else:\n",
    "        subfolder = [int(f) for f in os.listdir(dir_name) if is_a_number(f)]\n",
    "        if len(subfolder) > 0:\n",
    "            new_folder_number = np.max(subfolder) + 1\n",
    "        else:\n",
    "            new_folder_number = 1\n",
    "        output_folder = dir_name + \"/\" + str(new_folder_number)\n",
    "        os.mkdir(output_folder)\n",
    "    \n",
    "    print(\" - dataset loading\")\n",
    "    # Dataset loading\n",
    "    folder = dataset_folder + version_folder + \"/\" + feature \n",
    "    training_set=np.load(folder + '_training_set.npy')\n",
    "    validation_set=np.load(folder + '_validation_set.npy')\n",
    "    training_labels=np.load(folder + '_training_labels.npy')\n",
    "    validation_labels=np.load(folder + '_validation_labels.npy')\n",
    "    test_set=np.load(folder + '_test_set.npy')\n",
    "    test_labels=np.load(folder + '_test_labels.npy')\n",
    "    #words_name=np.load('words_name.npy')\n",
    "    \n",
    "    print(training_set.shape)\n",
    "    print(training_labels.shape)\n",
    "    \n",
    "    training_set = training_set.reshape((-1,training_set.shape[1],training_set.shape[2],1))\n",
    "    validation_set = validation_set.reshape((-1,validation_set.shape[1],validation_set.shape[2],1))\n",
    "    test_set = test_set.reshape((-1,test_set.shape[1],test_set.shape[2],1))\n",
    "    \n",
    "    # Training and validation shuffling\n",
    "    randomize = np.arange(len(training_set))\n",
    "    np.random.shuffle(randomize)\n",
    "    training_set = training_set[randomize]\n",
    "    training_labels = training_labels[randomize]\n",
    "    \n",
    "    randomize = np.arange(len(validation_set))\n",
    "    np.random.shuffle(randomize)\n",
    "    validation_set = validation_set[randomize]\n",
    "    validation_labels = validation_labels[randomize]\n",
    "    \n",
    "    final_path = results_folder + version_folder + \"/\" + network + \"/\" + feature + \"/final.npy\"\n",
    "    \n",
    "    if not os.path.isfile(final_path):\n",
    "        # Pre-training dataset creation\n",
    "        print(\" - pre-training dataset creation\")\n",
    "        percentage = 30\n",
    "\n",
    "        index1 = int(len(training_set)*percentage/100)\n",
    "        index2 = int(len(validation_set)*percentage/100)\n",
    "\n",
    "        training_set2 = training_set[0:index1]\n",
    "        validation_set2 = validation_set[0:index2]\n",
    "\n",
    "        training_labels2 = training_labels[0:index1]\n",
    "        validation_labels2 = validation_labels[0:index2]\n",
    "\n",
    "        if rounds != 0:\n",
    "            # Pre-training\n",
    "            print(\" - pre-training\")\n",
    "            tgp, tgp2, ogp, ogp2, final = IAHOS(rounds, method, limits, attempts, variables, iterations, Classifier,\n",
    "                               training_set2, training_labels2, validation_set2, validation_labels2, feature, batch_size=1024)\n",
    "\n",
    "            np.save(final_path, final)\n",
    "        else:\n",
    "            final = [32, 3, 1, 1, 16, 3, 1, 100, 0.1]\n",
    "    else:\n",
    "        final = np.load(final_path)\n",
    "        print(\" - pre-training already done!\")\n",
    "    \n",
    "    # Training\n",
    "    training_accuracy=[]\n",
    "    validation_accuracy=[]\n",
    "    test_scores=[]\n",
    "    optimizers = ['radam','sgd','rmsprop','adagrad','adadelta','adam','adamax','nadam']\n",
    "    final = np.reshape(final, (len(final), 1))\n",
    "\n",
    "    network_input = [training_set.shape[1], training_set.shape[2]]\n",
    "    epochs=100\n",
    "    j=0\n",
    "    test_scores = []\n",
    "    for optimizer in tqdm(optimizers):\n",
    "        mc = ModelCheckpoint(output_folder + '/best_model_' + optimizer + '.h5', monitor='val_loss', mode='min', verbose=0, save_best_only=True)\n",
    "        classifier=Classifier(0, network_input, training_labels.shape[1], feature, final, optimizer, op[j], 'train')\n",
    "        history=classifier.fit(training_set, training_labels, validation_data=[validation_set,validation_labels],\n",
    "                               epochs=epochs, batch_size=1024, verbose=2, callbacks=[mc])\n",
    "        training_accuracy.append(history.history['acc'])\n",
    "        validation_accuracy.append(history.history['val_acc'])\n",
    "        classifier=Classifier(0, network_input, training_labels.shape[1], feature, final, optimizer, op[j], 'test')\n",
    "        predictions = classifier.predict(test_set)\n",
    "        new_test_labels = np.zeros(test_labels.shape[0])\n",
    "        for i in range(test_labels.shape[0]):\n",
    "            new_test_labels[i] = np.argmax(test_labels[i])\n",
    "        y_pred = np.zeros(test_labels.shape[0])\n",
    "        for i in range(test_labels.shape[0]):\n",
    "            y_pred[i] = np.argmax(predictions[i])\n",
    "        score=accuracy_score(y_true=new_test_labels,y_pred=y_pred, normalize=True)\n",
    "        test_scores.append(score)\n",
    "        \n",
    "        np.save(output_folder + \"/training_accuracy_\" + optimizer + '.npy', training_accuracy)\n",
    "        np.save(output_folder + \"/validation_accuracy_\" + optimizer + '.npy', validation_accuracy)\n",
    "        np.save(output_folder + \"/new_test_labels_\" + optimizer + '.npy', new_test_labels)\n",
    "        np.save(output_folder + \"/y_pred_\" + optimizer + '.npy', y_pred)\n",
    "        \n",
    "        j+=1\n",
    "        if j<7:\n",
    "            K.clear_session()\n",
    "    \n",
    "    np.save(output_folder + \"/test_scores_\" + optimizer + '.npy', test_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = open(results_folder + version_folder + \"/\" + network + \"/test_results.txt\", \"w+\") \n",
    "for feature in features:\n",
    "    print(\"### \" + feature.upper() + \" ###\")\n",
    "    folder = dataset_folder + version_folder + \"/\" + feature\n",
    "    model_folder = results_folder + version_folder + \"/\" + network + \"/\" + feature\n",
    "    words_name=np.load(dataset_folder + version_folder + \"/\" + 'words_name.npy')\n",
    "    test_set=np.load(folder + '_test_set.npy')\n",
    "    test_labels=np.load(folder + '_test_labels.npy')\n",
    "    test_set = test_set.reshape((-1,test_set.shape[1],test_set.shape[2],1))\n",
    "    \n",
    "    dir_name = results_folder + version_folder + \"/\" + network + \"/\" + feature\n",
    "    subfolder = [int(f) for f in os.listdir(dir_name) if is_a_number(f)]\n",
    "    actual_folder = dir_name + \"/\" + str(subfolder[0])\n",
    "    files = [file for file in os.listdir(actual_folder)]\n",
    "    training_paths = [f for f in files if str(f).find(\"training_accuracy_\") >= 0]\n",
    "    validation_paths = [f for f in files if str(f).find(\"validation_accuracy_\") >= 0]\n",
    "    training_accuracies = np.array([np.load(actual_folder + \"/\" + p) for p in training_paths])[4]\n",
    "    validation_accuracies = np.array([np.load(actual_folder + \"/\" + p) for p in validation_paths])[4]\n",
    "    \n",
    "    plt.figure(figsize=(8, 8))\n",
    "    plt.plot(training_accuracies[0], label=\"radam\")\n",
    "    plt.plot(training_accuracies[1], label=\"sgd\")\n",
    "    plt.plot(training_accuracies[2], label=\"rmsprop\")\n",
    "    plt.plot(training_accuracies[3], label=\"adagrad\")\n",
    "    plt.plot(training_accuracies[4], label=\"adadelta\")\n",
    "    plt.plot(training_accuracies[5], label=\"adam\")\n",
    "    plt.plot(training_accuracies[6], label=\"adamax\")\n",
    "    plt.plot(training_accuracies[7], label=\"nadam\")\n",
    "    plt.title((feature + \" training accuracies\").upper())\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.ylabel(\"Accuracy\")\n",
    "    plt.xlim(-1, training_accuracies.shape[1])\n",
    "    plt.ylim(0.5, 1.05)\n",
    "    plt.legend()\n",
    "    plt.savefig(results_folder + version_folder + \"/\" + network + \"/\" + \"training_accuracies_\" + feature + \".jpg\")\n",
    "    \n",
    "    plt.figure(figsize=(8, 8))\n",
    "    plt.plot(validation_accuracies[0], label=\"radam\")\n",
    "    plt.plot(validation_accuracies[1], label=\"sgd\")\n",
    "    plt.plot(validation_accuracies[2], label=\"rmsprop\")\n",
    "    plt.plot(validation_accuracies[3], label=\"adagrad\")\n",
    "    plt.plot(validation_accuracies[4], label=\"adadelta\")\n",
    "    plt.plot(validation_accuracies[5], label=\"adam\")\n",
    "    plt.plot(validation_accuracies[6], label=\"adamax\")\n",
    "    plt.plot(validation_accuracies[7], label=\"nadam\")\n",
    "    plt.title((feature + \" validation accuracies\").upper())\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.ylabel(\"Accuracy\")\n",
    "    plt.xlim(-1, validation_accuracies.shape[1])\n",
    "    plt.ylim(0.5, 1.05)\n",
    "    plt.legend()\n",
    "    plt.savefig(results_folder + version_folder + \"/\" + network + \"/\" + \"validation_accuracies_\" + feature + \".jpg\")\n",
    "\n",
    "    optimizers = ['radam','sgd','rmsprop','adagrad','adadelta','adam','adamax','nadam']\n",
    "    final_path = results_folder + version_folder + \"/\" + network + \"/\" + feature + \"/final.npy\"\n",
    "    final = np.load(final_path)\n",
    "    final = np.reshape(final, (len(final), 1))\n",
    "    file.write(\"Final parameters = \" + str(final) + \"\\n\")\n",
    "    \n",
    "    file.write(\"### \" + feature.upper() + \" ###\\n\")\n",
    "    \n",
    "    network_input = [test_set.shape[1], test_set.shape[2]]\n",
    "    j=0\n",
    "    test_scores=[]\n",
    "    for optimizer in optimizers:\n",
    "        classifier=Classifier(0, network_input, test_labels.shape[1], feature, final, optimizer, op[j], 'test')\n",
    "        classifier.load_weights(model_folder + '/1/best_model_'+str(optimizer)+'.h5')\n",
    "        start_time = int(round(time.time() * 1000))\n",
    "        predictions = classifier.predict(test_set)\n",
    "        end_time = int(round(time.time() * 1000))\n",
    "        new_test_labels = np.zeros(test_labels.shape[0])\n",
    "        for i in range(test_labels.shape[0]):\n",
    "            new_test_labels[i]=np.argmax(test_labels[i])\n",
    "        y_pred = np.zeros(test_labels.shape[0])\n",
    "        for i in range(test_labels.shape[0]):\n",
    "            y_pred[i]=np.argmax(predictions[i])\n",
    "        score=accuracy_score(y_true=new_test_labels,y_pred=y_pred, normalize=True)\n",
    "        test_scores.append(score)\n",
    "        epoch = np.argmax(validation_accuracies[j])\n",
    "        print(optimizer + \" score = \" + str(score) + \" in epoch \" + str(epoch))\n",
    "        file.write(optimizer + \"val score = \" + str(score) + \" in epoch \" + str(epoch) + \", inference time = \" + (str(end_time - start_time)) + \" ms \\n\")\n",
    "        \n",
    "        r = results_folder + version_folder + \"/\" + network + \"/\"\n",
    "        plot_confusion_matrix(new_test_labels,y_pred,words_name,feature,optimizer,r)\n",
    "        \n",
    "        j+=1\n",
    "        del classifier\n",
    "    y = np.zeros((8,1))\n",
    "    for i in range(8):\n",
    "        y[i,0]=test_scores[i]\n",
    "        \n",
    "file.close() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
