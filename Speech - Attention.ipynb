{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Speech - Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from keras.models import Model, load_model\n",
    "\n",
    "from keras.layers import Input, Activation, Concatenate, Permute, Reshape, Flatten, Lambda, Dot, Softmax\n",
    "from keras.layers import Add, Dropout, BatchNormalization, Conv2D, Reshape, MaxPooling2D, Dense, CuDNNLSTM, Bidirectional\n",
    "from keras import backend as K\n",
    "from keras.utils import to_categorical\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint, LearningRateScheduler\n",
    "from keras import backend as K\n",
    "from keras import optimizers\n",
    "from keras_radam import RAdam\n",
    "\n",
    "import numpy as np\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.io import wavfile\n",
    "import plotly.graph_objects as go\n",
    "import plotly\n",
    "from plotly.subplots import make_subplots\n",
    "import winsound\n",
    "import time\n",
    "from keras.utils.vis_utils import plot_model\n",
    "\n",
    "from IAHOS import IAHOS\n",
    "from extraction_performances import extraction_performances\n",
    "from hyperparams_initialization import hyperparams_initialization\n",
    "from plots import plot_IAHOS,plot_confusion_matrix\n",
    "from plots import plot_training_accuracy,plot_validation_accuracy,plot_test_scores\n",
    "from plots import plot_output_NN\n",
    "\n",
    "import keras\n",
    "from keras import backend as K\n",
    "from keras.models import Sequential,Input,Model,load_model\n",
    "from keras.layers import Conv2D,Conv1D,MaxPooling2D,AveragePooling1D,MaxPooling1D\n",
    "from keras.layers import Dense,Flatten,Dropout\n",
    "from keras import initializers,optimizers,backend as k\n",
    "from keras_radam import RAdam\n",
    "from keras.callbacks import EarlyStopping,ModelCheckpoint\n",
    "from sklearn.metrics import confusion_matrix,accuracy_score\n",
    "import tensorflow as tf\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Classifier(network_input_shape, number_of_classes, optimizer, rnn_func = CuDNNLSTM):\n",
    "\n",
    "    inputs = Input(shape=(int(network_input_shape[0]), int(network_input_shape[1]), 1))\n",
    "    \n",
    "    x = Permute((2,1,3)) (inputs)\n",
    "    \n",
    "    x = Conv2D(10, (5,1) , activation='relu', padding='same') (x)\n",
    "    x = BatchNormalization() (x)\n",
    "    x = Conv2D(1, (5,1) , activation='relu', padding='same') (x)\n",
    "    x = BatchNormalization() (x)\n",
    "\n",
    "    x = Lambda(lambda q: K.squeeze(q, -1), name='squeeze_last_dim') (x) #keras.backend.squeeze(x, axis)\n",
    "\n",
    "    x = Bidirectional(rnn_func(64, return_sequences = True)) (x) # [b_s, seq_len, vec_dim]\n",
    "    x = Bidirectional(rnn_func(64, return_sequences = True)) (x) # [b_s, seq_len, vec_dim]\n",
    "\n",
    "    xFirst = Lambda(lambda q: q[:,49]) (x) #[b_s, vec_dim] #32\n",
    "    query = Dense(128) (xFirst)\n",
    "\n",
    "    #dot product attention\n",
    "    attScores = Dot(axes=[1,2])([query, x]) \n",
    "    attScores = Softmax(name='attSoftmax')(attScores) #[b_s, seq_len]\n",
    "\n",
    "    #rescale sequence\n",
    "    attVector = Dot(axes=[1,1])([attScores, x]) #[b_s, vec_dim]\n",
    "\n",
    "    x = Dense(64, activation = 'relu')(attVector)\n",
    "    x = Dense(32)(x)\n",
    "\n",
    "    output = Dense(number_of_classes, activation = 'softmax', name='output')(x)\n",
    "\n",
    "    model = Model(inputs=[inputs], outputs=[output])\n",
    "    \n",
    "    # Optimizer choice\n",
    "    if optimizer=='radam':\n",
    "        Optimizer=RAdam()\n",
    "    elif optimizer=='sgd':\n",
    "        Optimizer=keras.optimizers.SGD(lr=0.01, momentum=0.0, decay=0.0, nesterov=False)\n",
    "    elif optimizer=='rmsprop':\n",
    "        Optimizer=keras.optimizers.RMSprop(lr=0.001, rho=0.9, epsilon=None, decay=0.0)\n",
    "    elif optimizer=='adagrad':\n",
    "        Optimizer=keras.optimizers.Adagrad(lr=0.01, epsilon=None, decay=0.0)\n",
    "    elif optimizer=='adadelta':\n",
    "        Optimizer=keras.optimizers.Adadelta(lr=1.0, rho=0.95,epsilon=None, decay=0.0)\n",
    "    elif optimizer=='adam':\n",
    "        Optimizer=keras.optimizers.Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0, amsgrad=False)\n",
    "    elif optimizer=='adamax':\n",
    "        Optimizer=keras.optimizers.Adamax(lr=0.002, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0)\n",
    "    else:\n",
    "        Optimizer=keras.optimizers.Nadam(lr=0.002, beta_1=0.9, beta_2=0.999, epsilon=None, schedule_decay=0.004)\n",
    "        \n",
    "    model.compile(optimizer=Optimizer,loss='categorical_crossentropy',metrics=['accuracy'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_a_number(s):\n",
    "    try: \n",
    "        int(s)\n",
    "        return True\n",
    "    except ValueError:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "network = \"Attention\"\n",
    "features = [\"filter_banks\", \"mfcc\", \"mfcc_deltas\"]\n",
    "dataset_folder = './dataset/'\n",
    "results_folder = \"./results/\"\n",
    "version_folder = \"1567874784415\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for feature in features:\n",
    "    \n",
    "    print(\"### \" + feature.upper() + \" ###\")\n",
    "    \n",
    "    dir_name = results_folder + version_folder\n",
    "    if not os.path.exists(dir_name):\n",
    "        os.mkdir(dir_name)\n",
    "        \n",
    "    dir_name = dir_name + \"/\" + network\n",
    "    if not os.path.exists(dir_name):\n",
    "        os.mkdir(dir_name)\n",
    "    \n",
    "    dir_name = dir_name + \"/\" + feature\n",
    "    if not os.path.exists(dir_name):\n",
    "        os.mkdir(dir_name)\n",
    "        output_folder = dir_name + \"/1\"\n",
    "        os.mkdir(output_folder)\n",
    "    else:\n",
    "        subfolder = [int(f) for f in os.listdir(dir_name) if is_a_number(f)]\n",
    "        if len(subfolder) > 0:\n",
    "            new_folder_number = np.max(subfolder) + 1\n",
    "        else:\n",
    "            new_folder_number = 1\n",
    "        output_folder = dir_name + \"/\" + str(new_folder_number)\n",
    "        os.mkdir(output_folder)\n",
    "    \n",
    "    print(\" - dataset loading\")\n",
    "    # Dataset loading\n",
    "    folder = dataset_folder + version_folder + \"/\" + feature \n",
    "    training_set=np.load(folder + '_training_set.npy')\n",
    "    validation_set=np.load(folder + '_validation_set.npy')\n",
    "    training_labels=np.load(folder + '_training_labels.npy')\n",
    "    validation_labels=np.load(folder + '_validation_labels.npy')\n",
    "    test_set=np.load(folder + '_test_set.npy')\n",
    "    test_labels=np.load(folder + '_test_labels.npy')\n",
    "    #words_name=np.load('words_name.npy')\n",
    "    \n",
    "    training_set = training_set.reshape((-1,training_set.shape[1],training_set.shape[2],1))\n",
    "    validation_set = validation_set.reshape((-1,validation_set.shape[1],validation_set.shape[2],1))\n",
    "    test_set = test_set.reshape((-1,test_set.shape[1],test_set.shape[2],1))\n",
    "    \n",
    "    # Training and validation shuffling\n",
    "    randomize = np.arange(len(training_set))\n",
    "    np.random.shuffle(randomize)\n",
    "    training_set = training_set[randomize]\n",
    "    training_labels = training_labels[randomize]\n",
    "    \n",
    "    randomize = np.arange(len(validation_set))\n",
    "    np.random.shuffle(randomize)\n",
    "    validation_set = validation_set[randomize]\n",
    "    validation_labels = validation_labels[randomize]\n",
    "    \n",
    "    # Training\n",
    "    training_accuracy=[]\n",
    "    validation_accuracy=[]\n",
    "    test_scores=[]\n",
    "    optimizers = ['radam']\n",
    "\n",
    "    network_input = [training_set.shape[1], training_set.shape[2]]\n",
    "    epochs=50\n",
    "    j=0\n",
    "    for optimizer in tqdm(optimizers):\n",
    "        \n",
    "        mc = ModelCheckpoint(output_folder + '/best_model_' + optimizer + '.h5', monitor='val_loss', mode='min', verbose=0, save_best_only=True)\n",
    "        classifier = Classifier((training_set.shape[1], training_set.shape[2]), training_labels.shape[1], optimizer)\n",
    "        history=classifier.fit(training_set, training_labels, validation_data=[validation_set,validation_labels],\n",
    "                               epochs=epochs, batch_size=1024, verbose=2, callbacks=[mc])\n",
    "        training_accuracy.append(history.history['acc'])\n",
    "        validation_accuracy.append(history.history['val_acc'])\n",
    "        \n",
    "        np.save(output_folder + \"/training_accuracy_\" + optimizer + '.npy', training_accuracy)\n",
    "        np.save(output_folder + \"/validation_accuracy_\" + optimizer + '.npy', validation_accuracy)\n",
    "        \n",
    "        j+=1\n",
    "        if j<7:\n",
    "            K.clear_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### FILTER_BANKS ###\n",
      "radam score = 0.8781720430107527 in epoch 40\n",
      "### MFCC ###\n",
      "radam score = 0.89 in epoch 47\n",
      "### MFCC_DELTAS ###\n",
      "radam score = 0.8878494623655914 in epoch 41\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 720x576 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 720x576 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 720x576 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "file = open(results_folder + version_folder + \"/\" + network + \"/test_results.txt\", \"w+\") \n",
    "for feature in features:\n",
    "    print(\"### \" + feature.upper() + \" ###\")\n",
    "    folder = dataset_folder + version_folder + \"/\" + feature\n",
    "    model_folder = results_folder + version_folder + \"/\" + network + \"/\" + feature\n",
    "    words_name=np.load(dataset_folder + version_folder + \"/\" + 'words_name.npy')\n",
    "    test_set=np.load(folder + '_test_set.npy')\n",
    "    test_labels=np.load(folder + '_test_labels.npy')\n",
    "    test_set = test_set.reshape((-1,test_set.shape[1],test_set.shape[2],1))\n",
    "    \n",
    "    dir_name = results_folder + version_folder + \"/\" + network + \"/\" + feature\n",
    "    subfolder = [int(f) for f in os.listdir(dir_name) if is_a_number(f)]\n",
    "    actual_folder = dir_name + \"/\" + str(subfolder[0])\n",
    "    files = [file for file in os.listdir(actual_folder)]\n",
    "    training_paths = [f for f in files if str(f).find(\"training_accuracy_\") >= 0]\n",
    "    validation_paths = [f for f in files if str(f).find(\"validation_accuracy_\") >= 0]\n",
    "    \n",
    "    training_accuracies = np.array([np.load(actual_folder + \"/\" + p) for p in training_paths])[0]\n",
    "    validation_accuracies = np.array([np.load(actual_folder + \"/\" + p) for p in validation_paths])[0]\n",
    "    '''\n",
    "    plt.figure(figsize=(8, 8))\n",
    "    plt.plot(training_accuracies[0], label=\"radam\")\n",
    "    plt.plot(training_accuracies[1], label=\"sgd\")\n",
    "    plt.plot(training_accuracies[2], label=\"rmsprop\")\n",
    "    plt.plot(training_accuracies[3], label=\"adagrad\")\n",
    "    plt.plot(training_accuracies[4], label=\"adadelta\")\n",
    "    plt.plot(training_accuracies[5], label=\"adam\")\n",
    "    plt.plot(training_accuracies[6], label=\"adamax\")\n",
    "    plt.plot(training_accuracies[7], label=\"nadam\")\n",
    "    plt.title((feature + \" training accuracies\").upper())\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.ylabel(\"Accuracy\")\n",
    "    plt.xlim(-1, training_accuracies.shape[1])\n",
    "    plt.ylim(0.5, 1.05)\n",
    "    plt.legend()\n",
    "    plt.savefig(results_folder + version_folder + \"/\" + network + \"/\" + \"training_accuracies_\" + feature + \".jpg\")\n",
    "    \n",
    "    plt.figure(figsize=(8, 8))\n",
    "    plt.plot(validation_accuracies[0], label=\"radam\")\n",
    "    plt.plot(validation_accuracies[1], label=\"sgd\")\n",
    "    plt.plot(validation_accuracies[2], label=\"rmsprop\")\n",
    "    plt.plot(validation_accuracies[3], label=\"adagrad\")\n",
    "    plt.plot(validation_accuracies[4], label=\"adadelta\")\n",
    "    plt.plot(validation_accuracies[5], label=\"adam\")\n",
    "    plt.plot(validation_accuracies[6], label=\"adamax\")\n",
    "    plt.plot(validation_accuracies[7], label=\"nadam\")\n",
    "    plt.title((feature + \" validation accuracies\").upper())\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.ylabel(\"Accuracy\")\n",
    "    plt.xlim(-1, validation_accuracies.shape[1])\n",
    "    plt.ylim(0.5, 1.05)\n",
    "    plt.legend()\n",
    "    plt.savefig(results_folder + version_folder + \"/\" + network + \"/\" + \"validation_accuracies_\" + feature + \".jpg\")\n",
    "'''\n",
    "    optimizers = ['radam']\n",
    "    \n",
    "    \n",
    "    file.write(\"### \" + feature.upper() + \" ###\\n\")\n",
    "    \n",
    "    network_input = [test_set.shape[1], test_set.shape[2]]\n",
    "    j=0\n",
    "    test_scores=[]\n",
    "    for optimizer in optimizers:\n",
    "        classifier=Classifier(network_input, test_labels.shape[1], optimizer)\n",
    "        classifier.load_weights(model_folder + '/1/best_model_'+str(optimizer)+'.h5')\n",
    "        start_time = int(round(time.time() * 1000))\n",
    "        predictions = classifier.predict(test_set)\n",
    "        end_time = int(round(time.time() * 1000))\n",
    "        new_test_labels = np.zeros(test_labels.shape[0])\n",
    "        for i in range(test_labels.shape[0]):\n",
    "            new_test_labels[i]=np.argmax(test_labels[i])\n",
    "        y_pred = np.zeros(test_labels.shape[0])\n",
    "        for i in range(test_labels.shape[0]):\n",
    "            y_pred[i]=np.argmax(predictions[i])\n",
    "        score=accuracy_score(y_true=new_test_labels,y_pred=y_pred, normalize=True)\n",
    "        test_scores.append(score)\n",
    "        epoch = np.argmax(validation_accuracies[j])\n",
    "        print(optimizer + \" score = \" + str(score) + \" in epoch \" + str(epoch))\n",
    "        file.write(optimizer + \"val score = \" + str(score) + \" in epoch \" + str(epoch) + \", inference time = \" + (str(end_time - start_time)) + \" ms \\n\")\n",
    "        \n",
    "        r = results_folder + version_folder + \"/\" + network + \"/\"\n",
    "        plot_confusion_matrix(new_test_labels,y_pred,words_name,feature,optimizer,r)\n",
    "        \n",
    "        j+=1\n",
    "        del classifier\n",
    "    #y = np.zeros((8,1))\n",
    "    #for i in range(8):\n",
    "    #    y[i,0]=test_scores[i]\n",
    "        \n",
    "file.close() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
